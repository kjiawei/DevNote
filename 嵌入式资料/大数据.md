![Hadoop生态体系结构.png](https://upload-images.jianshu.io/upload_images/2636843-5528ad26bd2bba4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![设计数据库保存电影信息.png](https://upload-images.jianshu.io/upload_images/2636843-ca4e8a05d7845b47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

基础：1、什么是大数据？
	  2、大数据要解决核心问题？ 
		（1）数据存储 ---> 分布式文件系统 HDFS
		（2）数据计算 ---> 分布式计算 MapReduce（Yarn）

第七期上课：月底开课
上课时间：周一  三   五的晚上  8点   ~ 10点40
          一周8个课时
		  
周二 四  六的晚上 安排的练习
		
=====================================
一、当Oracle DataBase数据库遇到大数据
	1、如果对Oracle数据库有一定的了解，对于学习Hadoop很有帮助
	2、为什么大数据不采用关系型数据库（Oracle、MySQL）来存储数据？
	3、举例：设计一个数据库，来保存电影的信息   ---> MongoDB
	   举例：设计一个数据库，保存非结构化数据   ---> HBase、Redis

二、Hadoop生态体系结构

三、Hadoop实战（安装和部署）: 带着大家一起来装Hadoop
	1、安装Linux
		网卡：仅主机模式，要求虚拟机的IP跟本机在一个网段（192.168.157.1）
		
	2、配置Linux
		（1）关闭防火墙
		       systemctl stop firewalld.service   ----> 停用防火墙，但重启后，又启用防火墙
			   systemctl disable firewalld.service  永久关闭
		
		（2）设置主机名 编辑文件  vi /etc/hosts
		        192.168.157.81 hadoop81
				
		（3）安装JDK: 版本： JDK 1.8 64位
			解压  tar -zxvf jdk-8u144-linux-x64.tar.gz -C ~/training/
			设置环境变量：JAVA_HOME
			  vi ~/.bash_profile
				JAVA_HOME=/root/training/jdk1.8.0_144
				export JAVA_HOME

				PATH=$JAVA_HOME/bin:$PATH
				export PATH
				
			  生效环境变量：source ~/.bash_profile

	3、安装Hadoop: 版本：  2.7.3
		问题：Hadoop有几种安装方式？  3种
		配置文件的位置： $HADOOP_HOME/etc/hadoop
		准备工作：
		a、解压  tar -zxvf hadoop-2.7.3.tar.gz -C ~/training/
		b、设置Hadoop的环境变量
		    vi ~/.bash_profile
				HADOOP_HOME=/root/training/hadoop-2.7.3
				export HADOOP_HOME

				PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$PATH
				export PATH
			
			source ~/.bash_profile
			
		（1）本地模式:  一台
			特点：没有HDFS、只能测试MapReduce程序
			      数据就是Linux文件系统上的数据
			$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar
			例子：wordcount 单词计数
			hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount ~/temp/data/mydata.txt ~/temp/output
		
		（2）伪分布模式： 一台
			修改配置文件
				hdfs-site.xml文件
					<!--数据块的冗余度，默认：3-->
					<property>
					   <name>dfs.replication</name>
					   <value>1</value>
					</property>

					<!--禁用权限检查-->
					<property>
					   <name>dfs.permissions</name>
					   <value>false</value>
					</property>
					
				core-site.xml  配置NameNode地址和数据存储的目录

					<!--NameNode的地址-->				
					<property>
					   <name>fs.defaultFS</name>
					   <value>hdfs://hadoop81:9000</value>
					</property>		

					<!--HDFS保存数据块、元信息和日志的目录-->
					<property>
					   <name>hadoop.tmp.dir</name>
					   <value>/root/training/hadoop-2.7.3/tmp</value>
					</property>	
					
				mapred-site.xml  ---> 默认没有这个文件
				     cp mapred-site.xml.template mapred-site.xml

					<!--MapReduce运行的框架-->
					<property>
					   <name>mapreduce.framework.name</name>
					   <value>yarn</value>
					</property>	

				yarn-site.xml
	
					<!--ResourceManager的地址-->	
					<property>
					   <name>yarn.resourcemanager.hostname</name>
					   <value>hadoop81</value>
					</property>					

					<!--MapReduce运行的方式是：shuffle洗牌-->
					<property>
					   <name>yarn.nodemanager.aux-services</name>
					   <value>mapreduce_shuffle</value>
					</property>	

			格式化HDFS的NameNode：hdfs namenode -format
				日志：
				Storage directory /root/training/hadoop-2.7.3/tmp/dfs/name has been successfully formatted.
		
			启动Hadoop：start-all.sh 两部分  1、HDFS   2、Yarn
			停止Hadoop：stop-all.sh
		
				问题：1、启动一次需要输入4次密码
				      2、停止一次需要输入4次密码
		
		（3）全分布模式（需要几台机器？3台）
	
		注意：伪分布模式和全分布模式一定要配置免密码登录
